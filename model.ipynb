{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a ChatBot using TensorFlow with RNN and LSTM\n",
    "\n",
    "The assignment: \n",
    "\n",
    "1. Read the json file and create a proper data structure.\n",
    "2. Shuffle the data and create a 80-10-10 split (80% for training, 10% for development, and 10% for testing). \n",
    "3. Train a neural net. \n",
    "4. Create an api (use some Python framework like Flask, and remember to set an API key so that you protect the data).\n",
    "5. Write a Readme.md file with comments on what you have done, how to run the code, and how to deploy on Google Cloud.\n",
    "6. Upload to GitHub\n",
    "\n",
    "### My process for creating the TensorFlow model:\n",
    "1. Import the data from JSON\n",
    "2. Process the data into a suitable format\n",
    "3. Build the model\n",
    "4. Train the model\n",
    "5. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = json.load(open('data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_main = data['dialogues']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file has a lot of metadata, with the data I'm interested in lying in the dialogues section. To make this more intuitive I have gone into this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Preprocess the data\n",
    "The data is currently in a format that is not helpful for my deep learning model. My goals within this sections are to:\n",
    "\n",
    "- Create a list of questions whose corresponding answers is in a list of answers.\n",
    "- Convert these questions and answers into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Questions\n",
    "questions = []\n",
    "\n",
    "# Answers\n",
    "answers = []\n",
    "\n",
    "def sep_data(data):\n",
    "    for i in range(len(data)):\n",
    "        for key, value in data[i]['samples'].items():\n",
    "            for j in range(len(value)):\n",
    "                questions.append(value[j])\n",
    "                answers.append(list(data[i]['replies'].values())[0])\n",
    "\n",
    "sep_data(data_main)\n",
    "\n",
    "answers = [item for sublist in answers for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cleaning the text\n",
    "Here I will clean the text. That is to remove punctuation and convert english word abbreviations. Whilst my data is mostly Norwegian words, there are still English sentences with abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\",\"i am\", text)\n",
    "    text = re.sub(r\"he's\",\"he is\", text)\n",
    "    text = re.sub(r\"she's\",\"she is\", text)\n",
    "    text = re.sub(r\"that's\",\"that is\", text)\n",
    "    text = re.sub(r\"what's\",\"what is\", text)\n",
    "    text = re.sub(r\"where's\",\"where is\", text)\n",
    "    text = re.sub(r\"it's\",\"it is\", text)\n",
    "    text = re.sub(r\"\\'ll\",\" will\", text)\n",
    "    text = re.sub(r\"\\'ve\",\" have\", text)\n",
    "    text = re.sub(r\"\\'re\",\" are\", text)\n",
    "    text = re.sub(r\"\\'d\",\" would\", text)\n",
    "    text = re.sub(r\"won't\",\"will not\", text)\n",
    "    text = re.sub(r\"can't\",\"cannot\", text)\n",
    "    text = re.sub(r'[-()\\\"#/@;:<>{}+=~|.?,!]','',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question)) \n",
    "    \n",
    "clean_answers = []\n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Removing words that appear less than 5%\n",
    "I will now remove those words that appear less than 5% of the time, this is to improve the accuracy of my model. At the same time I will be creating a dictionary that maps each word to the amount of times that it appears in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2count = {}\n",
    "for question in clean_questions:\n",
    "    for word in question.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "\n",
    "for answer in clean_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 5\n",
    "questionswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold:\n",
    "        questionswords2int[word] = word_number\n",
    "        word_number += 1\n",
    "        \n",
    "answerswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold:\n",
    "        answerswords2int[word] = word_number\n",
    "        word_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Adding tokens\n",
    "I will now add the tokens to this dictionary. These will come in handy later on when we manipulate our strings further for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
    "\n",
    "for token in tokens:\n",
    "    questionswords2int[token] = len(questionswords2int) + 1\n",
    "    \n",
    "for token in tokens:\n",
    "    answerswords2int[token] = len(answerswords2int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flipping my answers dictionary from word - integer to integer - word\n",
    "\n",
    "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding EOS at the end of each answer\n",
    "\n",
    "for i in range(len(clean_answers)):\n",
    "    clean_answers[i] += \" <EOS>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Translating the words to integers\n",
    "As stated at the beginning of this section I need to translate all of my words into integers that correspond to that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions_into_int = []\n",
    "for question in clean_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questionswords2int:\n",
    "            ints.append(questionswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(questionswords2int[word])\n",
    "    questions_into_int.append(ints)\n",
    "answers_into_int = []\n",
    "for answer in clean_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answerswords2int:\n",
    "            ints.append(answerswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(answerswords2int[word])\n",
    "    answers_into_int.append(ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Sorting and clipping\n",
    "Now that I have my questions and answers in integer format, I will clip those that have a length over 25. Again this is to improve the accuracy of my model, as those sentences over 25 characters will be anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_clean_questions = []\n",
    "sorted_clean_answers = []\n",
    "\n",
    "for length in range(1, 25 + 1):\n",
    "    for i in enumerate(questions_into_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_clean_questions.append(questions_into_int[i[0]])\n",
    "            sorted_clean_answers.append(answers_into_int[i[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.0 Building the model\n",
    "The pre-processing of the data is now over, and I have a good structure to work with. Now I need to create a model to take this data and learn from it.\n",
    "\n",
    "My process for creating this model will be:\n",
    "1. Define model inputs\n",
    "2. Target preprocessing function\n",
    "3. Create the Encoder RNN layer\n",
    "4. Create Decoder RNN layer \n",
    "4. Define the Seq2Seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model inputs function\n",
    "I will begin here with the simple task of defining the placeholders for my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name = 'input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name = 'target')\n",
    "    lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    return inputs, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Preprocess targets\n",
    "This function will process my targets in a format for my model to understand, including by adding SOS (Start of sentence) at the beginning of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Will convert the target into the right format: with SOS at the beginning and in two batches\n",
    "def preprocess_targets(targets, word2int, batch_size):\n",
    "    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n",
    "    right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
    "    preprocessed_targets = tf.concat([left_side, right_side], 1)\n",
    "    return preprocessed_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Creating the Encoder RNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n",
    "                                                                    cell_bw = encoder_cell,\n",
    "                                                                    sequence_length = sequence_length,\n",
    "                                                                    inputs = rnn_inputs,\n",
    "                                                                    dtype = tf.float32)\n",
    "    return encoder_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.4 Create the Decoder RNN Layer \n",
    "This part of the model is fairly involved as I need to split this section into 3 parts to create my decoder RNN layer:\n",
    "1. Decode the training set\n",
    "2. Decode the validation set\n",
    "3. Decode the RNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decoding the training set\n",
    "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              name = \"attn_dec_train\")\n",
    "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                              training_decoder_function,\n",
    "                                                                                                              decoder_embedded_input,\n",
    "                                                                                                              sequence_length,\n",
    "                                                                                                              scope = decoding_scope)\n",
    "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
    "    return output_function(decoder_output_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decoding the test/validation set\n",
    "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
    "                                                                              encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              decoder_embeddings_matrix,\n",
    "                                                                              sos_id,\n",
    "                                                                              eos_id,\n",
    "                                                                              maximum_length,\n",
    "                                                                              num_words,\n",
    "                                                                              name = \"attn_dec_inf\")\n",
    "    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                                test_decoder_function,\n",
    "                                                                                                                scope = decoding_scope)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the Decoder RNN\n",
    "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "        weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
    "                                                                      num_words,\n",
    "                                                                      None,\n",
    "                                                                      scope = decoding_scope,\n",
    "                                                                      weights_initializer = weights,\n",
    "                                                                      biases_initializer = biases)\n",
    "        training_predictions = decode_training_set(encoder_state,\n",
    "                                                   decoder_cell,\n",
    "                                                   decoder_embedded_input,\n",
    "                                                   sequence_length,\n",
    "                                                   decoding_scope,\n",
    "                                                   output_function,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size)\n",
    "        decoding_scope.reuse_variables()\n",
    "        test_predictions = decode_test_set(encoder_state,\n",
    "                                           decoder_cell,\n",
    "                                           decoder_embeddings_matrix,\n",
    "                                           word2int['<SOS>'],\n",
    "                                           word2int['<EOS>'],\n",
    "                                           sequence_length - 1,\n",
    "                                           num_words,\n",
    "                                           decoding_scope,\n",
    "                                           output_function,\n",
    "                                           keep_prob,\n",
    "                                           batch_size)\n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Defining the seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int):\n",
    "    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs,\n",
    "                                                              answers_num_words + 1,\n",
    "                                                              encoder_embedding_size,\n",
    "                                                              initializer = tf.random_uniform_initializer(0, 1))\n",
    "    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
    "    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_embedding_size], 0, 1))\n",
    "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
    "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input,\n",
    "                                                         decoder_embeddings_matrix,\n",
    "                                                         encoder_state,\n",
    "                                                         questions_num_words,\n",
    "                                                         sequence_length,\n",
    "                                                         rnn_size,\n",
    "                                                         num_layers,\n",
    "                                                         questionswords2int,\n",
    "                                                         keep_prob,\n",
    "                                                         batch_size)\n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Training the model\n",
    "Our RNN model has been built, now it's time to train the model on my data.\n",
    "\n",
    "My process for training the model will be as follows:\n",
    "1. Initial set-up\n",
    "2. Define loss error, optimizer etc.\n",
    "3. Further string manipulation\n",
    "4. Training for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Initial set-up\n",
    "To set up my model training I want to achieve the following:\n",
    "- Define my hyperparameters\n",
    "- Define my session\n",
    "- Load the model inputs\n",
    "- Set length of sequence and the shape of the inputs\n",
    "- Get my training and test predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Setting hyperparameters\n",
    "Training a deep learning model takes time. I had to make a pay-off between optimal chatbot performance and training time. To strike a good balance I decided on the following parameters which resulted in a training time of roughly 3 hours. If I had more time then this model could be even more accurate by using better parameters and training within the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "rnn_size = 512\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate= 0.0001\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define my session\n",
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading model inputs\n",
    "inputs, targets, lr, keep_prob = model_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting sequence length\n",
    "sequence_length = tf.placeholder_with_default(25, None, name = 'sequence_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting the shape of inputs tensor\n",
    "input_shape = tf.shape(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting training and test predictions\n",
    "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
    "                                                       targets,\n",
    "                                                       keep_prob,\n",
    "                                                       batch_size,\n",
    "                                                       sequence_length,\n",
    "                                                       len(answerswords2int),\n",
    "                                                       len(questionswords2int),\n",
    "                                                       encoding_embedding_size,\n",
    "                                                       decoding_embedding_size,\n",
    "                                                       rnn_size,\n",
    "                                                       num_layers,\n",
    "                                                       questionswords2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Defining loss error, optimizer etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"optimization\"):\n",
    "    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,\n",
    "                                                  targets,\n",
    "                                                  tf.ones([input_shape[0], sequence_length]))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(loss_error)\n",
    "    clipped_gradients = [(tf.clip_by_value(grad_tensor, -5., 5.), grad_variable) for grad_tensor, grad_variable in gradients if grad_tensor is not None]\n",
    "    optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Further string manipulation\n",
    "Before I enter my training for loop I need to do some further string manipulation. Namely:\n",
    "\n",
    "- Add padding so each sentence is the same length\n",
    "- Split my data into the batches that will be fed into the model\n",
    "- Train, dev, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Padding the sequences with PAD\n",
    "def apply_padding(batch_of_sequences, word2int):\n",
    "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
    "    return [sequence + [word2int['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into batches of questions and answers\n",
    "def split_into_batches(questions, answers, batch_size):\n",
    "    for batch_index in range(0, len(questions) // batch_size):\n",
    "        start_index = batch_index * batch_size\n",
    "        questions_in_batch = questions[start_index : start_index + batch_size]\n",
    "        answers_in_batch = answers[start_index : start_index + batch_size]\n",
    "        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questionswords2int))\n",
    "        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answerswords2int))\n",
    "        yield padded_questions_in_batch, padded_answers_in_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Train, dev, test split\n",
    "One of my specific tasks for this project was to create a train, dev, test split that was 80,10,10. To do this I first created a train, test split using SkLearn (which also shuffled my data as part of the process) and then split that training data into train and development data. \n",
    "\n",
    "I am then left with three groups of data:\n",
    "1. Training Questions, Answers\n",
    "2. Validation Questions, Answers\n",
    "3. Questions Shuffled Testing, Answers Shuffled Testing\n",
    "\n",
    "The testing data can then be used later to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting questions and answers into training, validation, and test sets\n",
    "questions_shuffled, questions_shuffled_testing, answers_shuffled, answers_shuffled_testing = train_test_split(sorted_clean_questions,sorted_clean_answers,test_size=0.1,shuffle=True,random_state=101)\n",
    "\n",
    "# questions_shuffled, answers_shuffled - Will now split these into the testing and validation\n",
    "\n",
    "training_validation_split = int(len(questions_shuffled) * 0.1)\n",
    "training_questions = questions_shuffled[training_validation_split:]\n",
    "training_answers = answers_shuffled[training_validation_split:]\n",
    "validation_questions = questions_shuffled[:training_validation_split]\n",
    "validation_answers = answers_shuffled[:training_validation_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Training for loop\n",
    "This is the section where the model will learn my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/10, Batch:    0/63, Training Loss Error:  0.077, Training Time on 100 Batches: 1780 seconds\n",
      "Validation Loss Error:  1.326, Batch Validation Time: 35 seconds\n",
      "I speak better now!!\n",
      "Validation Loss Error:  1.249, Batch Validation Time: 35 seconds\n",
      "I speak better now!!\n",
      "Epoch:   2/10, Batch:    0/63, Training Loss Error:  1.405, Training Time on 100 Batches: 1667 seconds\n",
      "Validation Loss Error:  1.173, Batch Validation Time: 35 seconds\n",
      "I speak better now!!\n",
      "Validation Loss Error:  1.104, Batch Validation Time: 35 seconds\n",
      "I speak better now!!\n",
      "Epoch:   3/10, Batch:    0/63, Training Loss Error:  0.805, Training Time on 100 Batches: 1661 seconds\n",
      "Validation Loss Error:  1.038, Batch Validation Time: 34 seconds\n",
      "I speak better now!!\n",
      "Validation Loss Error:  0.973, Batch Validation Time: 35 seconds\n",
      "I speak better now!!\n",
      "Epoch:   4/10, Batch:    0/63, Training Loss Error:  0.717, Training Time on 100 Batches: 1653 seconds\n",
      "Validation Loss Error:  0.946, Batch Validation Time: 34 seconds\n",
      "I speak better now!!\n",
      "Validation Loss Error:  0.906, Batch Validation Time: 35 seconds\n",
      "I speak better now!!\n",
      "Epoch:   5/10, Batch:    0/63, Training Loss Error:  0.661, Training Time on 100 Batches: 1651 seconds\n",
      "Validation Loss Error:  0.873, Batch Validation Time: 35 seconds\n",
      "I speak better now!!\n",
      "Validation Loss Error:  0.828, Batch Validation Time: 34 seconds\n",
      "I speak better now!!\n",
      "Epoch:   6/10, Batch:    0/63, Training Loss Error:  0.613, Training Time on 100 Batches: 1650 seconds\n",
      "Validation Loss Error:  0.783, Batch Validation Time: 35 seconds\n",
      "I speak better now!!\n",
      "Validation Loss Error:  2.590, Batch Validation Time: 34 seconds\n",
      "Sorry I do not speak better, I need to practice more.\n",
      "Epoch:   7/10, Batch:    0/63, Training Loss Error:  0.903, Training Time on 100 Batches: 1583 seconds\n",
      "Validation Loss Error:  0.997, Batch Validation Time: 35 seconds\n",
      "Sorry I do not speak better, I need to practice more.\n",
      "Validation Loss Error:  0.980, Batch Validation Time: 35 seconds\n",
      "Sorry I do not speak better, I need to practice more.\n",
      "Epoch:   8/10, Batch:    0/63, Training Loss Error:  0.773, Training Time on 100 Batches: 1599 seconds\n",
      "Validation Loss Error:  0.837, Batch Validation Time: 34 seconds\n",
      "Sorry I do not speak better, I need to practice more.\n",
      "Validation Loss Error:  0.785, Batch Validation Time: 35 seconds\n",
      "Sorry I do not speak better, I need to practice more.\n",
      "Epoch:   9/10, Batch:    0/63, Training Loss Error:  0.603, Training Time on 100 Batches: 1583 seconds\n",
      "Validation Loss Error:  0.731, Batch Validation Time: 34 seconds\n",
      "I speak better now!!\n",
      "Validation Loss Error:  0.678, Batch Validation Time: 35 seconds\n",
      "I speak better now!!\n",
      "Epoch:  10/10, Batch:    0/63, Training Loss Error:  0.535, Training Time on 100 Batches: 1652 seconds\n",
      "Validation Loss Error:  0.620, Batch Validation Time: 35 seconds\n",
      "I speak better now!!\n",
      "Validation Loss Error:  0.575, Batch Validation Time: 34 seconds\n",
      "I speak better now!!\n",
      "Game Over\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "batch_index_check_training_loss = 100\n",
    "batch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1\n",
    "total_training_loss_error = 0\n",
    "list_validation_loss_error = []\n",
    "early_stopping_check = 0\n",
    "early_stopping_stop = 100\n",
    "checkpoint = \"chatbot_weights.ckpt\"\n",
    "session.run(tf.global_variables_initializer())\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n",
    "        starting_time = time.time()\n",
    "        _, batch_training_loss_error = session.run([optimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch,\n",
    "                                                                                               targets: padded_answers_in_batch,\n",
    "                                                                                               lr: learning_rate,\n",
    "                                                                                               sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                                               keep_prob: keep_probability})\n",
    "        total_training_loss_error += batch_training_loss_error\n",
    "        ending_time = time.time()\n",
    "        batch_time = ending_time - starting_time\n",
    "        if batch_index % batch_index_check_training_loss == 0:\n",
    "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
    "                                                                                                                                       epochs,\n",
    "                                                                                                                                       batch_index,\n",
    "                                                                                                                                       len(training_questions) // batch_size,\n",
    "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
    "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
    "            total_training_loss_error = 0\n",
    "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
    "            total_validation_loss_error = 0\n",
    "            starting_time = time.time()\n",
    "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\n",
    "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
    "                                                                       targets: padded_answers_in_batch,\n",
    "                                                                       lr: learning_rate,\n",
    "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                       keep_prob: 1})\n",
    "                total_validation_loss_error += batch_validation_loss_error\n",
    "            ending_time = time.time()\n",
    "            batch_time = ending_time - starting_time\n",
    "            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
    "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "            list_validation_loss_error.append(average_validation_loss_error)\n",
    "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
    "                print('I speak better now!!')\n",
    "                early_stopping_check = 0\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session, checkpoint)\n",
    "            else:\n",
    "                print(\"Sorry I do not speak better, I need to practice more.\")\n",
    "                early_stopping_check += 1\n",
    "                if early_stopping_check == early_stopping_stop:\n",
    "                    break\n",
    "    if early_stopping_check == early_stopping_stop:\n",
    "        print(\"My apologies, I cannot speak better anymore. This is the best I can do.\")\n",
    "        break\n",
    "print(\"Game Over\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Testing my chatbot\n",
    "My model has now run through the training and is ready to be implemented. This is a short section and will go through this process:\n",
    "\n",
    "1. Begin the session\n",
    "2. Convert question to integers\n",
    "3. Setting up the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the weights and Running the session\n",
    "checkpoint = \"./chatbot_weights.ckpt\"\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(session, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the questions to integers\n",
    "def convert_string2int(question, word2int):\n",
    "    question = clean_text(question)\n",
    "    return [word2int.get(word, word2int['<OUT>']) for word in question.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: hei\n",
      "ChatBot:  jeg er feil bot som rundt for.\n",
      "You: hva heter du\n",
      "ChatBot:  jeg er en chatbot.\n",
      "You: hvor bor du?\n",
      "ChatBot:  jeg er en chatbot og luken jeg.\n",
      "You: Hva er Convertelligence?\n",
      "ChatBot:  jeg er en chatbot.\n",
      "You: Liker du meg?\n",
      "ChatBot:  jeg er feil spiller en chatbot.\n",
      "You: What can you help me with?\n",
      "ChatBot:  jeg er feil bot til å svare på convertelligence på en chatbot som har en chatbot.\n",
      "You: hva kan du gjøre?\n",
      "ChatBot:  jeg er feil orkester.\n",
      "You: hva mener du?\n",
      "ChatBot:  jeg er alltid på en chatbot.\n",
      "You: Can you speak other languages?\n",
      "ChatBot:  jeg er feil bot som rundt.\n",
      "You: Hvem jobber du til>\n",
      "ChatBot:  jeg er feil spiller jeg er en chatbot.\n"
     ]
    }
   ],
   "source": [
    "# Setting up the chat\n",
    "while(True):\n",
    "    question = input(\"You: \")\n",
    "    if question == 'Goodbye':\n",
    "        break\n",
    "    question = convert_string2int(question, questionswords2int)\n",
    "    question = question + [questionswords2int['<PAD>']] * (25 - len(question))\n",
    "    fake_batch = np.zeros((batch_size, 25))\n",
    "    fake_batch[0] = question\n",
    "    predicted_answer = session.run(test_predictions, {inputs: fake_batch, keep_prob: 0.5})[0]\n",
    "    answer = ''\n",
    "    for i in np.argmax(predicted_answer, 1):\n",
    "        if answersints2word[i] == 'i':\n",
    "            token = ' I'\n",
    "        elif answersints2word[i] == '<EOS>':\n",
    "            token = '.'\n",
    "        elif answersints2word[i] == '<OUT>':\n",
    "            token = 'out'\n",
    "        else:\n",
    "            token = ' ' + answersints2word[i]\n",
    "        answer += token\n",
    "        if token == '.':\n",
    "            break\n",
    "    print('ChatBot: ' + answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
